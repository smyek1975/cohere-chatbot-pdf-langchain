{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0FyHWcc6RI9"
   },
   "source": [
    "## Lab 15. Article Recommender with Text Embedding, Classification, and Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGX0RfPUfFgq"
   },
   "source": [
    "Stack multiple NLP models together to get an output much closer to our desired outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkGvDUYd2gnh"
   },
   "source": [
    "- We will build a simple news article recommender system that computes the embeddings of all available articles and recommend the most relevant articles based on embeddings similarity. \n",
    "\n",
    "- We will also make the recommendation tighter by using text classification to recommend only articles within the same category. \n",
    "\n",
    "- We will then extract a list of tags from each recommended article, which can further help readers discover new articles. \n",
    "\n",
    "- All this will be done via three Cohere API endpoints stacked together: Embed, Classify, and Generate.\n",
    "\n",
    "![Article recommender with Embed, Classify, and Generate](images/article-rec-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNvRai38Isqr"
   },
   "source": [
    "We will implement the following steps:\n",
    "\n",
    "**1: Find the most similar articles to the one currently reading using embeddings.**\n",
    "\n",
    "**2: Keep only articles of the same category using text classification.**\n",
    "\n",
    "**3: Extract tags from these articles.**\n",
    "\n",
    "**4: Show the top 5 recommended articles.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9-RyLu7KHII"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import cohere\n",
    "api_key = 'tRGKPmSqy0QKq2f2LbB120INzMd1wYbP9s896Dl3'\n",
    "\n",
    "# Create and retrieve a Cohere API key from os.cohere.ai\n",
    "co = cohere.Client(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeR8yeXl4YtQ"
   },
   "source": [
    "# **1. Find the most similar articles to the one currently reading using embeddings**\n",
    "\n",
    "![Step 1 - Embed](images/article-rec-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Efe2XFpWqsw7"
   },
   "source": [
    "## 1.1: Get articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJMuR_Pb5TUh"
   },
   "source": [
    "We'll use the [BBC news article dataset](https://www.kaggle.com/competitions/learn-ai-bbc/data?select=BBC+News+Train.csv) as an example [[Source]](http://mlg.ucd.ie/datasets/bbc.html). This dataset consists of articles from a few categories: business, politics, tech, entertainment, and sport.\n",
    "\n",
    "We'll extract a subset of the data and in Step 1, use the first 100 data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "G95txfh1KHCu",
    "outputId": "3b58f535-d1b9-4862-f6d1-e2aa5e37495c"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/cohere-ai/notebooks/main/notebooks/data/bbc_news_subset.csv', delimiter=',')\n",
    "\n",
    "# Select a portion of the dataset \n",
    "INP_START = 0\n",
    "INP_END = 100\n",
    "df_inputs = df.iloc[INP_START:INP_END]\n",
    "df_inputs = df_inputs.copy()\n",
    "\n",
    "# Remove columns we don't need\n",
    "df_inputs.drop(['ArticleId','Category'],axis=1,inplace=True)\n",
    "\n",
    "# View the data\n",
    "df_inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_kIeQpENJzp"
   },
   "source": [
    "## 1.2: Turn articles into embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu3fRvs3A5oe"
   },
   "source": [
    "Turn each article text into embeddings. An [embedding](https://docs.cohere.ai/embedding-wiki) is a list of numbers that our models use to represent a piece of text, capturing its context and meaning.\n",
    "\n",
    "We do this by calling Cohere's [Embed endpoint](https://docs.cohere.ai/embed-reference), which takes in text as input and returns embeddings as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoLcyJLsKHAN",
    "outputId": "d3e53c90-9505-4a5d-ef65-135312b7b0eb"
   },
   "outputs": [],
   "source": [
    "articles = df_inputs['Text'].tolist()\n",
    "\n",
    "output = co.embed(\n",
    "            model ='embed-english-v3.0',\n",
    "            input_type='search_document',\n",
    "            texts = articles)\n",
    "embeds = output.embeddings\n",
    "\n",
    "print('Number of articles:', len(embeds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqOIWI_LM2Rc"
   },
   "source": [
    "## 1.3: Pick one article and find the most similar articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIKvKKgJBcnK"
   },
   "source": [
    "Next, we pick any one article to be the one the reader is currently reading (let's call this the target) and find other articles with the most similar embeddings (let's call these candidates) using cosine similarity.\n",
    "\n",
    "[Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is a metric that measures how similar sequences of numbers are (embeddings in our case), and we compute it for each target-candidate pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfJJZ6Z0NaWg",
    "outputId": "40baea0c-9d1e-4fa2-a75e-3b0f0ba491ce"
   },
   "outputs": [],
   "source": [
    "# Choose one article ID as the one you are currently reading\n",
    "print(f'Choose one article ID between {INP_START} and {INP_END-1} below...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yn0lvlBuNwq_"
   },
   "outputs": [],
   "source": [
    "# Enter your article ID\n",
    "READING_IDX = 70\n",
    "\n",
    "# Get embedding for the article\n",
    "reading = embeds[READING_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhhaiIsEQF9k"
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between the target and candidate articles\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_similarity(target,candidates):\n",
    "  # Turn list into array\n",
    "  candidates = np.array(candidates)\n",
    "  target = np.expand_dims(np.array(target),axis=0)\n",
    "\n",
    "  # Calculate cosine similarity\n",
    "  similarity_scores = cosine_similarity(target,candidates)\n",
    "  similarity_scores = np.squeeze(similarity_scores).tolist()\n",
    "\n",
    "  # Sort by descending order in similarity\n",
    "  similarity_scores = list(enumerate(similarity_scores))\n",
    "  similarity_scores = sorted(similarity_scores, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "  # Return similarity scores\n",
    "  return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVNQ0cUDR-5U",
    "outputId": "040e57ad-ff14-4c19-97f1-554edc4dac59"
   },
   "outputs": [],
   "source": [
    "# Get the similarity between the target and candidate articles\n",
    "similarity = get_similarity(reading,embeds)\n",
    "\n",
    "# View the top 5 articles\n",
    "print('Target:')\n",
    "print(f'[ID {READING_IDX}]',df_inputs['Text'][READING_IDX][:100],'...','\\n')\n",
    "\n",
    "print('Candidates:')\n",
    "for i in similarity[1:6]: # Exclude the target article\n",
    "  print(f'[ID {i[0]}]',df_inputs['Text'][i[0]][:100],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpeETEhfyvOa"
   },
   "source": [
    "# **2: Keep only articles of the same category using text classification**\n",
    "\n",
    "![Step 2 - Classify](images/article-rec-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-neV3KEyyQP"
   },
   "source": [
    "In the example above (Article ID 70 as the target), we see that the top 5 most similar articles given by the system are very relevant. The target is a football/soccer article, and the system duly recommended very similar articles despite this dataset also containing articles from other sports like tennis and rugby.\n",
    " \n",
    "The fourth recommended article (ID 86) is not a sports article, but rather politics. Reading the text, it's likely because the target is news about a clash of individuals (i.e. anger about a racism fine), which happens to be what that politics article is also about (i.e. disagreement over an apology). So these two articles' meanings are similar in this way, captured in the embeddings.\n",
    " \n",
    "Perhaps we can make the system better by only recommending articles of the same category. For this, let's build a news category classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viuvclW_fqdf"
   },
   "source": [
    "## 2.1: Build a classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GFksI1A8mHS"
   },
   "source": [
    "We use Cohere's [Classify endpoint](https://docs.cohere.ai/classify-reference) to build a news category classifier, classifying articles into five categories: Business, Politics, Tech, Entertainment, and Sport. \n",
    "\n",
    "A typical text classification model requires hundreds/thousands of data points to train, but with this endpoint, we can build a classifier with a few as five examples per class.\n",
    "\n",
    "To build the classifier, we need a set of examples consisting of text (news text) and labels (news category). The BBC News dataset happens to have both (columns 'Text' and 'Category'), so this time weâ€™ll use the categories for building our examples. For this, we will set aside another portion of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pBJ5SQ5-zrJr",
    "outputId": "84a72cd9-4a8b-491e-c8da-4a8f46da4a99"
   },
   "outputs": [],
   "source": [
    "# Select a portion of the dataset to sample the classification examples from\n",
    "EX_START = 100\n",
    "EX_END = 200\n",
    "df_examples = df.iloc[EX_START:EX_END]\n",
    "df_examples = df_examples.copy()\n",
    "\n",
    "# Remove columns we don't need\n",
    "df_examples.drop(['ArticleId'],axis=1,inplace=True)\n",
    "\n",
    "# View the data\n",
    "df_examples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e2_yP0t-ciE"
   },
   "source": [
    "With the Classify endpoint, there is a limit of 2048 tokens with the medium model. This means full articles won't be able to fit in the examples, so we will approximate and limit each article to its first 300 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7l_B_87-dfT"
   },
   "outputs": [],
   "source": [
    "# Shorten the example articles (because the medium endpoint max token limit is 2048)\n",
    "MAX_CHARS = 300\n",
    "\n",
    "def shorten_text(text):\n",
    "  return text[:MAX_CHARS]\n",
    "\n",
    "df_examples['Text'] = df_examples['Text'].apply(shorten_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkqfMw019fvH"
   },
   "source": [
    "The Classify endpoint needs a minimum of 2 examples for each category. We'll have 5 examples each, sampled randomly from the dataset. We have 5 categories, so we will have a total of 25 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUdZ6y3g-ZEN",
    "outputId": "69076931-fa30-4602-cbb0-f9f4b45dc86f"
   },
   "outputs": [],
   "source": [
    "# Set the number of examples per category\n",
    "EX_PER_CAT = 5 \n",
    "\n",
    "# Get the list of all available categories\n",
    "categories = df_examples['Category'].unique().tolist()\n",
    "\n",
    "# Create list of examples containing texts and labels\n",
    "ex_texts = []\n",
    "ex_labels = []\n",
    "for category in categories:\n",
    "  df_category = df_examples[df_examples['Category'] == category]\n",
    "  samples = df_category.sample(n=EX_PER_CAT, random_state=42)\n",
    "  ex_texts += samples['Text'].tolist()\n",
    "  ex_labels += samples['Category'].tolist()\n",
    "\n",
    "print(f'Number of examples per category: {EX_PER_CAT}')\n",
    "print(f'List of categories: {categories}')\n",
    "print(f'Number of categories: {len(categories)}')\n",
    "print(f'Total number of examples: {len(ex_texts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuFzl0IS--SA"
   },
   "source": [
    "Once the examples are ready, we can now get the classifications. Here is a function that returns the classification given an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNFPu99N_Agc"
   },
   "outputs": [],
   "source": [
    "# Get classifications via the Classify endpoint\n",
    "\n",
    "from cohere.responses.classify import Example\n",
    "\n",
    "# Collate the examples\n",
    "examples = []\n",
    "for txt, lbl in zip(ex_texts,ex_labels):\n",
    "  examples.append(Example(txt,lbl))\n",
    "\n",
    "# Classification function\n",
    "def classify_text(texts, examples):\n",
    "    classifications = co.classify(\n",
    "        inputs=texts,\n",
    "        examples=examples\n",
    "    )\n",
    "    return [c.predictions[0] for c in classifications]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ3HJ8Ep4uYk"
   },
   "source": [
    "## 2.2: Measure its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spu8JHGX_C68"
   },
   "source": [
    "Before actually using the classifier, let's first test its performance. Here we take another 100 data points as the test dataset and the classifier will predict its class i.e. news category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-OwI7MzZ0Ev0",
    "outputId": "66d1ccf1-98db-4962-b313-3548cba5649b"
   },
   "outputs": [],
   "source": [
    "# Select a portion of the dataset for testing the classifier\n",
    "TEST_START = 200\n",
    "TEST_END = 300\n",
    "df_test = df.iloc[TEST_START:TEST_END]\n",
    "df_test = df_test.copy()\n",
    "\n",
    "# Remove columns we don't need\n",
    "df_test.drop(['ArticleId'],axis=1,inplace=True)\n",
    "\n",
    "# Shorten the text to fit token limit\n",
    "df_test['Text'] = df_test['Text'].apply(shorten_text)\n",
    "\n",
    "# View the data\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kkt3E5_c_O5c"
   },
   "outputs": [],
   "source": [
    "# Create batches of texts and classify them\n",
    "predictions = []\n",
    "BATCH_SIZE = 90 # The API accepts a maximum of 96 inputs\n",
    "for i in range(0, len(df_test['Text']), BATCH_SIZE):\n",
    "    batch_texts = df_test['Text'][i:i+BATCH_SIZE].tolist()\n",
    "    predictions.extend(classify_text(batch_texts, examples))\n",
    "    \n",
    "    \n",
    "# Actual classes\n",
    "actual = df_test['Category'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZ10tnuB_QKf",
    "outputId": "f3dc05c5-6dc3-456f-dc17-ca81fd69e323"
   },
   "outputs": [],
   "source": [
    "# Compute metrics on the test dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(actual, predictions)\n",
    "print(f'Accuracy: {accuracy*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0Rqzz1w_SiB"
   },
   "source": [
    "We get a good accuracy score of 91%, so the classifier is ready to be \n",
    "implemented in our recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaY7t_1DJBiy"
   },
   "source": [
    "# **3: Extract tags from these articles.**\n",
    "\n",
    "![Step 3 - Extract](images/article-rec-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8hoJCh7Tv01"
   },
   "source": [
    "We now proceed to the tags extraction step. Compared to the previous two steps, this step is not about sorting or filtering articles, but rather enriching them with more information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tIhfHWO0JBRd"
   },
   "outputs": [],
   "source": [
    "# Create the base prompt containing extraction examples\n",
    "base_prompt = 'Given a news article, please return the list tags containing proper nouns.\\n\\nArticle: japanese banking battle at an end japan s sumitomo mitsui financial has withdrawn its takeover offer for rival bank ufj holdings  enabling the latter to merge with mitsubishi tokyo.  sumitomo bosses told counterparts at ufj of its decision on friday  clearing the way for it to conclude a 3 trillion\\n\\nTags: sumitomo mitsui financial, ufj holdings, mitsubishi tokyo\\n--\\nArticle: france starts digital terrestrial france has become the last big european country to launch a digital terrestrial tv (dtt) service.  initially  more than a third of the population will be able to receive 14 free-to-air channels. despite the long wait for a french dtt roll-out  the new platform s bac\\n\\nTags: france\\n--\\nArticle: apple laptop is  greatest gadget  the apple powerbook 100 has been chosen as the greatest gadget of all time  by us magazine mobile pc.  the 1991 laptop was chosen because it was one of the first  lightweight  portable computers and helped define the layout of all future notebook pcs. the magazine h\\n\\nTags: apple, apple powerbook 100, mobile pc\\n--\\nArticle:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqg40uJbXs9T"
   },
   "source": [
    "We call the endpoint by specifying a few settings, and it will generate the corresponding extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6j0t4VjJJBNp"
   },
   "outputs": [],
   "source": [
    "# Get extractions via the Generate endpoint\n",
    "def extract_tags(article):\n",
    "  prompt = f\"\"\"Given an article, extract a list of tags containing keywords \\\n",
    "of that article.\\n\\nArticle: japanese banking battle at an end japan s sumitomo mitsui \\\n",
    "financial has withdrawn its takeover offer for rival bank ufj holdings  enabling the \\\n",
    "latter to merge with mitsubishi tokyo.  sumitomo bosses told counterparts at ufj of its \\\n",
    "decision on friday  clearing the way for it to conclude a 3 trillion\\n\\nTags: \\\n",
    "sumitomo mitsui financial, ufj holdings, mitsubishi tokyo, japanese banking\\n--\\nArticle: \\\n",
    "france starts digital terrestrial france has become the last big european country to \\\n",
    "launch a digital terrestrial tv (dtt) service.  initially  more than a third of the \\\n",
    "population will be able to receive 14 free-to-air channels. despite the long wait for a \\\n",
    "french dtt roll-out  the new platform s bac\\n\\nTags: france, digital terrestrial\\n--\\nArticle: \\\n",
    "apple laptop is  greatest gadget  the apple powerbook 100 has been chosen as the greatest \\\n",
    "gadget of all time  by us magazine mobile pc.  the 1991 laptop was chosen because it was \\\n",
    "one of the first  lightweight  portable computers and helped define the layout of all future \\\n",
    "notebook pcs. the magazine h\\n\\nTags: apple, apple powerbook 100, laptop\\n--\\nArticle:{article}\\n\\nTags:\"\"\"\n",
    "  \n",
    "  \n",
    "  prediction = co.generate(\n",
    "    model='command',\n",
    "    prompt=prompt,\n",
    "    max_tokens=50,\n",
    "    temperature=0.3)\n",
    "\n",
    "  return prediction.generations[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzcXdkhR6hVy"
   },
   "source": [
    "# **4: Show the top 5 recommended articles.**\n",
    "\n",
    "![Complete all steps](images/article-rec-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLyUW3Gp4KRB"
   },
   "source": [
    "Let's now put everything together for our article recommender system.\n",
    "\n",
    "First, we select the target article and compute the similarity scores against the candidate articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XVTSzK6AXoS3",
    "outputId": "f1f690fb-d281-41f5-abb3-8eafa4c5a172"
   },
   "outputs": [],
   "source": [
    "# Choose one article ID as the one you are currently reading\n",
    "print(f'Choose one article ID between {INP_START} and {INP_END-1} below...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mM7mBSGvJBKd"
   },
   "outputs": [],
   "source": [
    "# Enter your article ID\n",
    "READING_IDX = 70\n",
    "\n",
    "# Get embedding for the article\n",
    "reading = embeds[READING_IDX]\n",
    "\n",
    "# Get the similarity between the target and candidate articles\n",
    "similarity = get_similarity(reading,embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECFlqp2k5jET"
   },
   "source": [
    "Next, we filter the articles via classification. Finally, we extract the keywords from each article and show the recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eviOxPUZJBGo"
   },
   "outputs": [],
   "source": [
    "# Choose the number of articles to recommend\n",
    "SHOW_TOP = 5\n",
    "\n",
    "# Shorten the text to fit token limit\n",
    "df_inputs = df_inputs.copy()\n",
    "df_inputs['Text'] = df_inputs['Text'].apply(shorten_text)\n",
    "\n",
    "# Get the recommendations\n",
    "def get_recommendations(reading_idx,similarity,show_top):\n",
    "\n",
    "  # Show the current article\n",
    "  print('------  You are reading...  ------')\n",
    "  print(f'[ID {READING_IDX}] Article:',df_inputs['Text'][reading_idx][:MAX_CHARS]+'...\\n')\n",
    "\n",
    "  # Show the recommended articles\n",
    "  print('------  You might also like...  ------')\n",
    "\n",
    "  # Classify the target article\n",
    "  target_class = classify_text([df_inputs['Text'][reading_idx]],examples) \n",
    "\n",
    "  count = 0\n",
    "  for idx,score in similarity:\n",
    "\n",
    "    # Classify each candidate article\n",
    "    candidate_class = classify_text([df_inputs['Text'][idx]],examples)\n",
    "\n",
    "    # Show recommendations\n",
    "    if target_class == candidate_class and idx != reading_idx:\n",
    "      selection = df_inputs['Text'][idx][:MAX_CHARS]\n",
    "      print(f'[ID {idx}] Article:',selection+'...')\n",
    "\n",
    "      # Extract and show tags\n",
    "      tags = extract_tags(selection)\n",
    "      if tags:\n",
    "          print(f'Tags: {tags.strip()}\\n')\n",
    "      else:\n",
    "          print(f'Tags: none\\n')      \n",
    "\n",
    "      # Increment the article count\n",
    "      count += 1\n",
    "\n",
    "      # Stop once articles reach the SHOW_TOP number\n",
    "      if count == show_top:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "im9BmZYJ614y",
    "outputId": "9c54028f-c404-4111-ccfc-c603d62c45ac"
   },
   "outputs": [],
   "source": [
    "# Show the recommended articles\n",
    "get_recommendations(READING_IDX,similarity,SHOW_TOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVhljS_4ILM2"
   },
   "source": [
    "Keeping to the Section 1.3 example, here we see how the classification and extraction steps have improved our recommendation outcome.\n",
    "\n",
    "First, now the politics article (ID 86) doesn't get recommended anymore. And now we have the tags related to each article being generated. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_Mkm7c7_X5D"
   },
   "source": [
    "Let's try a couple of other articles in business and tech and see the output..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNQpkgXvkNtI"
   },
   "source": [
    "Business article (returning recommendations around German economy and economic growth/slump):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "utbxE-k7_U1c",
    "outputId": "ca7b6085-6360-46a7-ed39-1c367e3317ef"
   },
   "outputs": [],
   "source": [
    "# A business news article example (ID 40)\n",
    "\n",
    "# Enter your article ID\n",
    "READING_IDX = 1\n",
    "\n",
    "# Get embedding for the article\n",
    "reading = embeds[READING_IDX]\n",
    "\n",
    "# Get the similarity between the target and candidate articles\n",
    "similarity = get_similarity(reading,embeds)\n",
    "\n",
    "# Show the recommended articles\n",
    "get_recommendations(READING_IDX,similarity,SHOW_TOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYitPtmJkUo5"
   },
   "source": [
    "Tech article (returning recommendations around consumer devices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGpZxk0j_6G4",
    "outputId": "d287c789-52ac-4cfe-92e3-3b605570a876"
   },
   "outputs": [],
   "source": [
    "# A tech news article example (ID 30)\n",
    "\n",
    "# Enter your article ID\n",
    "READING_IDX = 71\n",
    "\n",
    "# Get embedding for the article\n",
    "reading = embeds[READING_IDX]\n",
    "\n",
    "# Get the similarity between the target and candidate articles\n",
    "similarity = get_similarity(reading,embeds)\n",
    "\n",
    "# Show the recommended articles\n",
    "get_recommendations(READING_IDX,similarity,SHOW_TOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f55W2I9RZzXz"
   },
   "source": [
    "This demonstrates an example of how we can stack multiple NLP endpoints together to get an output much closer to our desired outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Article Recommender with Text Embedding, Classification, and Extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1fb8019e3560b882083e525615cf48e713d3a7345a15eb723d805e91aa410aac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
